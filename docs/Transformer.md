---
title: Transformer
aliases: ["transformer", "트랜스포머", "attention", "어텐션", "llm"]
---

# Transformer

> 문장 전체를 한 번에 보면서 단어 간 관계를 파악하는 방식

[[딥러닝]]에서 시퀀스 데이터를 처리하는 아키텍처다. 2017년 구글이 "Attention is All You Need" 논문에서 발표했다. [[GPT]], [[BERT]], [[Claude]] 같은 현대 [[LLM]]의 핵심 구조다.

기존 [[RNN]], [[LSTM]]은 문장을 앞에서 뒤로 순차적으로 읽었다. 트랜스포머는 문장 전체를 한 번에 보고 단어들 사이의 관계를 계산한다. 덕분에 병렬 처리가 가능해져서 학습 속도가 훨씬 빨라졌고, 긴 문장에서도 앞뒤 문맥을 잘 파악한다.

## 동작 원리

책을 읽을 때 모르는 단어가 나오면 문장 전체를 훑어보면서 의미를 파악한다. "그가 **은행**에 갔다"에서 은행이 금융기관인지 강가인지 알려면 주변 단어를 봐야 한다. 트랜스포머도 비슷하게 동작한다.

**핵심은 Attention(어텐션) 메커니즘이다.**

1. 문장의 각 단어를 숫자 벡터로 변환한다 ([[임베딩]])
2. 각 단어에서 Query(질문), Key(키), Value(값) 세 가지 벡터를 만든다
3. 한 단어의 Query를 다른 모든 단어의 Key와 비교해서 "얼마나 관련 있는지" 점수를 계산한다
4. 이 점수를 가중치로 써서 Value들을 합친다
5. 결과적으로 각 단어는 문맥을 반영한 새로운 벡터가 된다

비유하자면, 회의에서 발표자(Query)가 청중(Key)에게 질문을 던지고, 관련 있는 사람들(높은 점수)의 의견(Value)을 더 많이 반영하는 것과 비슷하다.

**Self-Attention을 여러 번 병렬로 수행한다 (Multi-Head Attention).**

한 번만 Attention 하면 한 가지 관점에서만 본다. 여러 개의 "머리(head)"가 각각 다른 관점에서 관계를 파악한다. 어떤 머리는 문법적 관계를, 어떤 머리는 의미적 관계를 학습한다.

**Encoder와 Decoder 구조다.**

- Encoder: 입력 문장을 이해해서 압축된 표현으로 만든다
- Decoder: 그 표현을 보면서 출력 문장을 생성한다

[[BERT]]는 Encoder만 쓰고, [[GPT]]는 Decoder만 쓴다. 번역 같은 작업은 둘 다 쓴다.

**Positional Encoding으로 순서 정보를 준다.**

트랜스포머는 문장을 동시에 보기 때문에 단어 순서를 모른다. 그래서 각 위치에 고유한 패턴의 숫자를 더해서 "이 단어는 몇 번째"라는 정보를 넣어준다. sin/cos 함수를 쓰는 방식이 원래 논문의 방법이고, 요즘은 학습 가능한 위치 임베딩을 쓰기도 한다.

## 용도

거의 모든 현대 [[NLP]] 모델이 트랜스포머 기반이다.

- [[GPT]] 시리즈 - 텍스트 생성, 챗봇
- [[BERT]] - 문장 분류, 감정 분석, 검색 순위
- [[T5]] - 번역, 요약, QA
- [[Claude]], [[ChatGPT]] - 대화형 AI

NLP를 넘어서 다른 분야에도 쓰인다.

- [[ViT|Vision Transformer]] - 이미지 분류
- [[DALL-E]], [[Stable Diffusion]] - 이미지 생성
- [[AlphaFold]] - 단백질 구조 예측
- 음성 인식, 추천 시스템, 시계열 예측

## 장단점

**장점**

- 병렬 처리 가능해서 [[GPU]]로 학습이 빠르다
- 긴 문장에서도 먼 거리의 단어 관계를 잘 파악한다 (RNN은 뒤로 갈수록 앞 정보를 잊음)
- 범용적이다. 텍스트, 이미지, 오디오 등 여러 도메인에 적용 가능
- 스케일링이 잘 된다. 모델 크기, 데이터, 컴퓨팅을 늘리면 성능이 계속 좋아진다

**단점**

- 계산량이 시퀀스 길이의 제곱에 비례한다. 입력이 길어지면 메모리와 시간이 폭발적으로 늘어난다
- 긴 문맥 처리가 제한적이다. 최근에는 [[Sparse Attention]], [[Flash Attention]], [[Mamba]] 같은 기법으로 해결하려 한다
- 학습에 엄청난 컴퓨팅 자원이 필요하다. [[GPT-4]] 같은 모델은 수천 개의 GPU로 몇 달씩 학습한다
- 왜 그렇게 동작하는지 해석하기 어렵다 (블랙박스)

## 주의점

**컨텍스트 길이 한계**: 기본 트랜스포머는 입력 길이에 제한이 있다. [[GPT-3]]는 4096 토큰, [[GPT-4]]는 8192~128K 토큰. 긴 문서를 처리하려면 청킹하거나 [[RAG]] 같은 기법을 써야 한다.

**Positional Encoding 이해 필수**: 위치 정보를 어떻게 주느냐에 따라 성능이 달라진다. [[RoPE]], [[ALiBi]] 같은 상대적 위치 인코딩이 요즘 트렌드.

**Attention은 만능이 아니다**: Attention 패턴을 시각화해보면 항상 의미 있는 관계를 학습하는 건 아니다. 그냥 통계적 패턴을 찾는 것일 수 있다.

**컴퓨팅 비용**: 추론할 때도 비용이 크다. 토큰 하나 생성할 때마다 전체 시퀀스에 대해 Attention을 계산해야 한다. [[KV Cache]]로 최적화하지만 메모리를 많이 먹는다.

## 생태계

- [[Hugging Face Transformers]] - 가장 많이 쓰는 라이브러리. [[PyTorch]], [[TensorFlow]] 지원
- [[PyTorch]] - 트랜스포머 구현할 때 주로 씀
- [[JAX]] / [[Flax]] - 구글에서 대규모 모델 학습할 때 씀
- [[vLLM]], [[TensorRT-LLM]] - 추론 최적화
- [[OpenAI API]], [[Anthropic API]] - 직접 학습 안 하고 API로 사용

## 기타

"Attention is All You Need" 논문 제목은 기존 방식을 비꼬는 의미가 있었다. 당시 [[RNN]], [[LSTM]]이 주류였는데, "그런 거 필요 없고 Attention만 있으면 된다"는 도발적인 주장이었다. 실제로 맞았다. 지금은 RNN을 쓰는 곳이 거의 없다.

논문 저자 8명 중 7명이 구글 브레인 소속이었다. 이 중 몇 명은 구글을 떠나서 스타트업을 차렸다. [[Cohere]], [[Adept]] 같은 회사들. "트랜스포머 마피아"라고 부른다. 한 논문이 산업 전체를 바꾼 케이스.

[[GPT]]의 "G"는 Generative(생성). Decoder만 쓴다. [[BERT]]의 "B"는 Bidirectional. Encoder만 쓴다. 둘 다 트랜스포머 기반인데 구조가 다르다. GPT는 다음 단어 예측, BERT는 문장 이해에 특화. 용도에 따라 나눠 쓴다.

트랜스포머 이후 AI 발전 속도가 폭발적이다. 2017년 논문 발표, 2018년 [[BERT]], 2020년 [[GPT-3]], 2022년 [[ChatGPT]], 2023년 [[GPT-4]]. 불과 몇 년 만에 인간 수준 대화가 가능해졌다. "AI의 iPhone 모멘트"라고 부른다.
